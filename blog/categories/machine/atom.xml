<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine | EOF]]></title>
  <link href="http://jasonleaster.github.io/blog/categories/machine/atom.xml" rel="self"/>
  <link href="http://jasonleaster.github.io/"/>
  <updated>2015-12-30T20:21:57+08:00</updated>
  <id>http://jasonleaster.github.io/</id>
  <author>
    <name><![CDATA[Jason Leaster]]></name>
    <email><![CDATA[jasonleaster@163.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Machine Learning With Boosting]]></title>
    <link href="http://jasonleaster.github.io/blog/2015/12/13/machine-learning-with-boosting/"/>
    <updated>2015-12-13T17:00:47+08:00</updated>
    <id>http://jasonleaster.github.io/blog/2015/12/13/machine-learning-with-boosting</id>
    <content type="html"><![CDATA[<p>This blog will talk about the theory and implementation about famouse
concept in machine learning &ndash; <code>Boosting</code>.</p>

<p>All algorithms are implemented in Python.</p>

<p>There are two main tasks which people want to finished with Machine Learning</p>

<ul>
<li>Classification</li>
<li>Regression</li>
</ul>


<p>There are a lot of other ways to do it but now we focus on boosting algorithm. You know that it&rsquo;s a fantastic way to make our work done.</p>

<h3>Adaboost for classification</h3>

<p>If you never hear about adaboost, I recommend you to finish the 7-th lab in MIT 6.034. It will help you a lot to understand what I&rsquo;m taking about. But this lab didn&rsquo;t build adaboost completely. So, I implement it individually.</p>

<p>Give the input training samples which have tag with it.</p>

<p>T = {(x1, y1), (x2, y2) &hellip; (xN, yN)}</p>

<p>where x_i is the feature of the i-th sample point and y_i is the <code>label</code> (soemtimes we call it as <code>tag</code>) with the sample point.</p>

<p>In this algorithm, there are only to different label of samples {-1, +1}.</p>

<p>Some classifier like dicision tree also can finish work about classification. But it&rsquo;s also easy to overfitting. So, we can&rsquo;t use it in some special situation. Instread of using decision tree, we use <code>decision stump</code>.</p>

<p><code>Yoav Freund</code> and <code>Robert Schapire</code> create this algorithm <strong>AdaBoost</strong> which means adaptive boosting. It combine some weaker classifier into a stronger classifier to avoid overfitting.</p>

<p>Test case:</p>

<p>There are training points with two different label. What if we input a point which&rsquo;s type is unkown, what the result will be?</p>

<p><img src="/images/img_for_2015_12_13/samples.png" alt="images" /></p>

<p>The test result is below there:</p>

<p><img src="/images/img_for_2015_12_13/result.png" alt="images" /></p>

<p>Just create a object of class <code>Adaboost</code> with your training samples with label. like this:</p>

<pre><code class="Python">import adaboost
a = AdaBoost(Original_Data, Tag)
# The up bound of training time to avoid the algorithm won't stop for not meeting the training accuracy.
times = 5 

a.train(times)

a.prediction(UnkownPoints)
</code></pre>

<p>API <code>prediction()</code> of class AdaBoost will return the result of prediction according to the model. All job done.</p>

<p>You could find other test case in my repository in github.</p>

<p><a href="https://github.com/jasonleaster/Machine_Learning/tree/master/Adaboost">Implementation of Adaboost in Python</a></p>

<p>You can check the validity of my implementation by the accuracy.</p>

<p><img src="/images/img_for_2015_12_13/samples2.png" alt="images" /></p>

<p>The data used to train the weaker classifier and get that accuracy is showed below there :</p>

<p><img src="/images/img_for_2015_12_13/accuracy.png" alt="images" /></p>

<h3>Boosting Tree</h3>

<p>We have knew to use <code>AdaBoost</code> to do classification. <code>Boosting Tree</code> will help us to do regression.</p>

<p>We also use decision stump as the weak classifier. But implementation of decision stump in this algorithm is not the same as that in AdaBoost.</p>

<p>There are ten samples in my test module:</p>

<pre><code class="python">Original_Data = numpy.array([
        [2],
        [3],
        [4],
        [5],
        [6],
        [7],
        [8],
        [9],
        [10],
        [1]
        ]).transpose()

ExpVal = numpy.array([
        [5.70],
        [5.91],
        [6.40],
        [6.80],
        [7.05],
        [8.90],
        [8.70],
        [9.00],
        [9.05],
        [5.56]
        ]).transpose()
</code></pre>

<p>The expected value of Original_Data[i] is ExpVal[i]. The input is from 1 to 10. How about to predict the output when the input is 1 or 11?</p>

<p>Let&rsquo;s test it. Here is the result:
<img src="/images/img_for_2015_12_13/output_of_boosting_tree.png" alt="images" /></p>

<p>Just used 11 weak classifier to construct a stronger classifier to do the regressio. The output is reasonable.</p>

<p>Here is my implementation of <code>Boosting Tree</code>
<a href="https://github.com/jasonleaster/Machine_Learning/tree/master/Boosting_Tree">Implementation of Boosting Tree in Python</a></p>

<p>Reference:</p>

<ol>
<li>MIT-6.034, Artificial Intelligence.</li>
<li>&lt;&lt; The statistic methods >> by HangLi.</li>
<li>Wikipedia.</li>
</ol>


<hr />

<p>Photo by Jason Leaster</p>

<p><img src="/images/img_for_2015_12_13/street.png" alt="images" /></p>
]]></content>
  </entry>
  
</feed>
