<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machinelearning | EOF]]></title>
  <link href="http://jasonleaster.github.io/blog/categories/machinelearning/atom.xml" rel="self"/>
  <link href="http://jasonleaster.github.io/"/>
  <updated>2016-09-05T23:36:32+08:00</updated>
  <id>http://jasonleaster.github.io/</id>
  <author>
    <name><![CDATA[Jason Leaster]]></name>
    <email><![CDATA[jasonleaster@163.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Training Cascade With OpenCV]]></title>
    <link href="http://jasonleaster.github.io/blog/2016/05/19/training-cascade-with-opencv/"/>
    <updated>2016-05-19T15:45:42+08:00</updated>
    <id>http://jasonleaster.github.io/blog/2016/05/19/training-cascade-with-opencv</id>
    <content type="html"><![CDATA[<p>Platform: Linux/Ubuntu</p>

<p>Preparation:</p>

<p>You may have to prepare two different types of images for training a <code>Binary Classifier Model</code>, the positive samples and the negative samples.</p>

<p>Here, we gona to use image database from <strong>UIUC Image Database for Car Detection</strong> to demonstrate how to use OpenCV to detection cars in a image.</p>

<!-- more -->

<p>User should put all positive samples which have the same size into a directory.</p>

<pre><code>ls ./pos &gt; ./pos_list.info
ls ./neg &gt; ./neg_list.info
</code></pre>

<p>Open <code>pos_list.info</code> and you will see the path of images have been written into the info file.</p>

<p><img src="/images/img_for_2016_05_19/files.png" alt="images" /></p>

<p>But it isn’t enough. To train the cascade with OpenCV, you should supply with the information where is the object in the image. In this demo, what we want to detect is a car. The information that OpenCV need like this: <code>image_path num x y w h</code>, which should be append at the end of the path of a image.</p>

<p><code>x y w h</code> describe a rectangle which identify where is the object that we want to find. <code>num</code> describe how many objects in the rectangle.</p>

<p>So, I write a script in Python and this script will help to finish that job.</p>

<p>``` python</p>

<p>[trans_pos_location.py]
fileObj = open(“./pos_list.info”)
newFile = open(“./pos_list_new.info”, “a+”)</p>

<p>for line in fileObj:
    newFile.write(“./pos/” + line[:-1] + “ 1 0 0 100 40\n”)</p>

<p>fileObj.close()
newFile.close()</p>

<p>[trans_neg_location.py]
import os</p>

<p>fileObj = open(“./neg_list.info”)
newFile = open(“./neg_list_new.info”, “a+”)</p>

<p>for line in fileObj:
    newFile.write(os.getcwd() + “/neg/” + line)</p>

<p>fileObj.close()
newFile.close()</p>

<p>```</p>

<p>Run the following comand:</p>

<pre><code>opencv_createsamples -info pos_list_new.info -num 550 -w 48 -h 24 -vec cars.vec

opencv_traincascade -data data -vec abc.vec -bg neg_list_new.info -numPos 550 -numNeg 500 -numStages 2 -w 48 -h 24
</code></pre>

<p><code>opencv_createsamples</code> and <code>opencv_traincascade</code> are two tool program with OpenCV. The original positive samples for training are images with 100x40 pixels. For the training process, it will cost a lot of memory, so we resize it into smaller one. With that command, <code>-w 48 -h 24</code>, positive images are resized into smaller images which’s width is 48 pixels and the height of that is 24 pixels.</p>

<p>Here, we can use this script to test the model that we get.</p>

<p>``` python</p>

<p>import cv2
import numpy
from matplotlib import image</p>

<p>car_cascade = cv2.CascadeClassifier(“/home/jasonleaster/Desktop/CarData/TrainImages/data/cascade.xml”)</p>

<p>gray = image.imread(“/home/jasonleaster/Desktop/CarData/TestImages_Scale/test-1.pgm”)</p>

<p>faces = car_cascade.detectMultiScale(gray,
                                    scaleFactor = 1.3,
                                    minNeighbors=5,
                                    minSize=(24, 48),
                                    flags = cv2.cv.CV_HAAR_SCALE_IMAGE)</p>

<p>img = gray</p>

<p>for (x, y, w, h) in faces:
    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)</p>

<p>from matplotlib import pyplot
import pylab
pyplot.imshow(img, cmap = “gray”)
pylab.show()</p>

<p>```</p>

<p>Result:</p>

<p><img src="/images/img_for_2016_05_19/detected_car1.png" alt="images" /></p>

<p>Reference:
1. http://blog.csdn.net/wuxiaoyao12/article/details/39227189
2. www.youtube.com/watch?v=WEzm7L5zoZE</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning With K-Means]]></title>
    <link href="http://jasonleaster.github.io/blog/2015/12/30/machine-learning-with-k-means/"/>
    <updated>2015-12-30T16:03:13+08:00</updated>
    <id>http://jasonleaster.github.io/blog/2015/12/30/machine-learning-with-k-means</id>
    <content type="html"><![CDATA[<p>K-Means is a classical unsupervised clustering Learning Algorithm. The detail of the theory about K-Means that you can find it in Wikipedia. Now I introduce to implement this algorithm by myself.</p>

<p>If you are interesting in the implementation and change it into a better version, you could find it in my github repository and give me some advices. I will be appreciated.</p>

<hr />

<p>So consider about if I want to classify the data into three different cluster. How could I make it?</p>

<p><img src="/images/img_for_2015_12_30/original.png" alt="images" /></p>

<p>Here is the result:</p>

<p><img src="/images/img_for_2015_12_30/result.png" alt="images" /></p>

<p>With the mean values:</p>

<p>``` python</p>

<p>Means =
[[3.5       1       6]
 [1.66666   6       6]]</p>

<p>```</p>

<!-- more -->

<p>In the implementation, I just choose the euclidean distance equation as my sensor to calculate the distance between samples. You could assign the <code>self.distance</code> with your function which is in your application.</p>

<p>Here, I show you how to classify the sample point in <code>K-Means</code>.</p>

<p>``` python</p>

<p>def classify(self):
    for i in range(self.SampleNum):
        minDis = +numpy.inf
        label  = None
        for k in range(self.classNum):
            d = self.distance(self._Mat[:, i].tolist(), self.meanVal[:, k].tolist())
            if d &lt; minDis:
                minDis = d
                label  = k</p>

<pre><code>    self.classification[i][0] = label
    self.classification[i][1] = minDis
</code></pre>

<p>```
And, here you will glance at the main procesure of this algorithm.</p>

<p>``` python</p>

<pre><code>"""
After you initialized this class, just call this
function and K Means Model will be built
"""
def train(self):
    while True:

        if self.stopOrNot():
            return

        self.classify()

        for k in range(self.classNum):
            mean    = None
            counter = 0
            for i in range(self.SampleNum):
                if self.classification[i][0] == k:
                    if mean == None:
                        mean =  numpy.array(self._Mat[:, i])*1.
                    else:
                        mean += self._Mat[:, i]

                    counter += 1.

            mean /= counter
            self.meanVal[:, k] = mean
</code></pre>

<p>```</p>

<p>Hope my work will help you in some day. Thank you.</p>

<p>Yous, EOF</p>

<hr />
<p>Photo by Annabella</p>

<p>Aha! Look! What a big shark. I’m fighting …</p>

<p><img src="/images/img_for_2015_12_30/bigshark.png" alt="images" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Labs of MIT 6.034]]></title>
    <link href="http://jasonleaster.github.io/blog/2015/12/25/labs-of-mit-6-dot-034/"/>
    <updated>2015-12-25T23:30:06+08:00</updated>
    <id>http://jasonleaster.github.io/blog/2015/12/25/labs-of-mit-6-dot-034</id>
    <content type="html"><![CDATA[<p>Two month ago, I decide to do a project about machine learning. So, I get start to learn machine learning and implemment some algorithms of ML.
I find that there is a course in MIT – 6.034 Artificial Intelligence which contain a lot of funny labs. That will help me to learn ML.</p>

<p>I don’t want to write a lot of analysis article about this course like what I have done in 6.008 . So, I just push my solution which also is incompletement onto github. If anyone interesting in this lab, you could touch me and I would like to communicate with you about these labs if I’m not busy.</p>

<p>What you should know is that the version of this labs is updated in 2015.</p>

<p>Here is the link you could get my solution:
<a href="https://github.com/jasonleaster/MIT_6.034_2015">My Solution of MIT 6.034</a></p>

<!-- more -->

<hr />
<p>Photo by Jason Leaster</p>

<p>Thanks god. There is a beautiful girl in my life and encourage me walk through the dark time of my life. Thank you. Annabella.</p>

<p><img src="/images/img_for_2015_12_25/girlfriend.png" alt="images" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning With Boosting]]></title>
    <link href="http://jasonleaster.github.io/blog/2015/12/13/machine-learning-with-boosting/"/>
    <updated>2015-12-13T17:00:47+08:00</updated>
    <id>http://jasonleaster.github.io/blog/2015/12/13/machine-learning-with-boosting</id>
    <content type="html"><![CDATA[<p>This blog will talk about the theory and implementation about famouse
concept in machine learning – <code>Boosting</code>.</p>

<p>All algorithms are implemented in Python.</p>

<p>There are two main tasks which people want to finish with Machine Learning.</p>

<ul>
  <li>Classification</li>
  <li>Regression</li>
</ul>

<p>There are a lot of other ways to do it but now we focus on <code>boosting</code> algorithm. You know that it’s a fantastic way to make our work done.</p>

<h3 id="adaboost-for-classification">Adaboost for classification</h3>

<p>If you never hear about adaboost, I recommend you to finish the 7-th lab in MIT 6.034. It will help you a lot to understand what I’m taking about. But this lab didn’t build adaboost completely. So, I implement it individually.</p>

<p>Give the input training samples which have tag with it.</p>

<div style="text-align:center"><img src="http://jasonleaster.github.io/images/img_for_2015_12_13/equation.png" align="middle" /> </div>

<p>where x[i] is the feature of the i-th sample point and y[i] is the <code>label</code> (soemtimes we call it as <code>tag</code>) with the sample point.</p>

<p>In this algorithm, there are only two different label of samples {-1, +1}.</p>

<p>Some classifier like decision tree also can work correctly about classification. But it’s also easy to overfitting. So, we can’t use it in some special situation. Instread of using decision tree, we use <code>decision stump</code> which is a special type of decision tree which’s depth is only one. So we call it as <code>decision stump</code>.</p>

<div style="text-align:center"><img src="http://jasonleaster.github.io/images/img_for_2015_12_13/stump.png" align="middle" /> </div>

<div style="text-align:center"><img src="http://jasonleaster.github.io/images/img_for_2015_12_13/weakClassifier.png" align="middle" /> </div>

<p><code>Yoav Freund</code> and <code>Robert Schapire</code> create this algorithm <strong>AdaBoost</strong> which means adaptive boosting.</p>

<div style="text-align:center"><img src="http://jasonleaster.github.io/images/img_for_2015_12_13/AdaBoost.png" align="middle" /> </div>

<p>Test case:</p>

<p>There are training points with two different label. What if we input a point which’s type is unkown, what the result will be?</p>

<p><img src="/images/img_for_2015_12_13/samples.png" alt="images" /></p>

<p>The test result is below there:</p>

<p><img src="/images/img_for_2015_12_13/result.png" alt="images" /></p>

<p>Just create a object of class <code>Adaboost</code> with your training samples with label. like this:</p>

<p>``` python</p>

<p>import adaboost
a = AdaBoost(Original_Data, Tag)
# The up bound of training time to avoid the algorithm won’t stop for not meeting the training accuracy.
times = 5</p>

<p>a.train(times)</p>

<p>a.prediction(UnkownPoints)</p>

<p>```</p>

<p>API <code>prediction()</code> of class AdaBoost will return the result of prediction according to the model. All job done.</p>

<p>You could find other test case in my repository in github.</p>

<p><a href="https://github.com/jasonleaster/Machine_Learning/tree/master/Adaboost">Implementation of Adaboost in Python</a></p>

<p>There is an <a href="http://cs229.stanford.edu/extra-notes/boosting.pdf">assignment</a> about AdaBoost in Stanford CS 229, which will ask student to implement stump booster. But I don’t really understand the skeleton of that source code. I think there must be something worng with that matlab script <code>stump_booster.m</code>. The week classifier can’t lost the direction information.</p>

<p>``` matlab</p>

<p>%%% !!! Don’t forget the important variable – direction</p>

<p>API given by the course materials:
function [ind, thresh] = find_best_threshold(X, y, p_dist)
function [theta, feature_inds, thresholds] = stump_booster(X, y, T)</p>

<p>API of my solution:
function [ind, thresh, direction] = find_best_threshold(X, y, p_dist)
function [theta, feature_inds, thresholds, directions] = stump_booster(X, y, T)</p>

<p>```</p>

<p>Run <code>boost_example.m</code>, you will see the <strong>classifier line</strong> with different iteration.</p>

<div style="text-align:center"><img src="http://jasonleaster.github.io/images/img_for_2015_12_13/iter2.jpg" width="400" height="400" align="middle" /> </div>

<div style="text-align:center"><img src="http://jasonleaster.github.io/images/img_for_2015_12_13/iter5.jpg" width="400" height="400" align="middle" /> </div>

<div style="text-align:center"><img src="http://jasonleaster.github.io/images/img_for_2015_12_13/iter10.jpg" width="400" height="400" align="middle" /> </div>

<h3 id="boosting-tree">Boosting Tree</h3>

<p>We have knew to use <code>AdaBoost</code> to do classification. <code>Boosting Tree</code> will help us to do regression.</p>

<p>We also use decision stump as the weak classifier. But implementation of decision stump in this algorithm is not the same as that in AdaBoost.</p>

<p>There are ten samples in my test module:</p>

<p>``` python</p>

<p>Original_Data = numpy.array([
        [2],
        [3],
        [4],
        [5],
        [6],
        [7],
        [8],
        [9],
        [10],
        [1]
        ]).transpose()</p>

<p>ExpVal = numpy.array([
        [5.70],
        [5.91],
        [6.40],
        [6.80],
        [7.05],
        [8.90],
        [8.70],
        [9.00],
        [9.05],
        [5.56]
        ]).transpose()</p>

<p>```</p>

<p>The expected value of Original_Data[i] is ExpVal[i]. The input is from 1 to 10. How about to predict the output when the input is 1 or 11?</p>

<p>Let’s test it. Here is the result:
<img src="/images/img_for_2015_12_13/output_of_boosting_tree.png" alt="images" /></p>

<p>Just used 11 weak classifier to construct a stronger classifier to do the regressio. The output is reasonable.</p>

<p>Here is my implementation of <code>Boosting Tree</code>
<a href="https://github.com/jasonleaster/Machine_Learning/tree/master/Boosting_Tree">Implementation of Boosting Tree in Python</a></p>

<p>Reference:</p>

<ol>
  <li>MIT-6.034, Artificial Intelligence. Lab-7</li>
  <li>« The statistic methods » by HangLi.</li>
  <li><a href="https://en.wikipedia.org/wiki/AdaBoost">Wikipedia</a></li>
</ol>

<hr />

<p>Photo by Jason Leaster</p>

<p><img src="/images/img_for_2015_12_13/street.png" alt="images" /></p>
]]></content>
  </entry>
  
</feed>
