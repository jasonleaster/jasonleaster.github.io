<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machinelearning | EOF]]></title>
  <link href="http://jasonleaster.github.io/blog/categories/machinelearning/atom.xml" rel="self"/>
  <link href="http://jasonleaster.github.io/"/>
  <updated>2016-02-03T23:12:39+08:00</updated>
  <id>http://jasonleaster.github.io/</id>
  <author>
    <name><![CDATA[Jason Leaster]]></name>
    <email><![CDATA[jasonleaster@163.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Machine Learning With K-Means]]></title>
    <link href="http://jasonleaster.github.io/blog/2015/12/30/machine-learning-with-k-means/"/>
    <updated>2015-12-30T16:03:13+08:00</updated>
    <id>http://jasonleaster.github.io/blog/2015/12/30/machine-learning-with-k-means</id>
    <content type="html"><![CDATA[<p>K-Means is a classical unsupervised clustering Learning Algorithm. The detail of the theory about K-Means that you can find it in Wikipedia. Now I introduce to implement this algorithm by myself.</p>

<p>If you are interesting in the implementation and change it into a better version, you could find it in my github repository and give me some advices. I will be appreciated.</p>

<hr />

<p>So consider about if I want to classify the data into three different cluster. How could I make it?</p>

<p><img src="/images/img_for_2015_12_30/original.png" alt="images" /></p>

<p>Here is the result:</p>

<p><img src="/images/img_for_2015_12_30/result.png" alt="images" /></p>

<p>With the mean values:
<code>python
Means =
[[3.5       1       6]
 [1.66666   6       6]]
</code></p>

<!-- more -->


<p>In the implementation, I just choose the euclidean distance equation as my sensor to calculate the distance between samples. You could assign the <code>self.distance</code> with your function which is in your application.</p>

<p>Here, I show you how to classify the sample point in <code>K-Means</code>.</p>

<pre><code class="python">def classify(self):
    for i in range(self.SampleNum):
        minDis = +numpy.inf
        label  = None
        for k in range(self.classNum):
            d = self.distance(self._Mat[:, i].tolist(), self.meanVal[:, k].tolist())
            if d &lt; minDis:
                minDis = d
                label  = k

        self.classification[i][0] = label
        self.classification[i][1] = minDis
</code></pre>

<p>And, here you will glance at the main procesure of this algorithm.
``` python</p>

<pre><code>"""
After you initialized this class, just call this
function and K Means Model will be built
"""
def train(self):
    while True:

        if self.stopOrNot():
            return

        self.classify()

        for k in range(self.classNum):
            mean    = None
            counter = 0
            for i in range(self.SampleNum):
                if self.classification[i][0] == k:
                    if mean == None:
                        mean =  numpy.array(self._Mat[:, i])*1.
                    else:
                        mean += self._Mat[:, i]

                    counter += 1.

            mean /= counter
            self.meanVal[:, k] = mean
</code></pre>

<p>```</p>

<p>Hope my work will help you in some day. Thank you.</p>

<p>Yous, EOF</p>

<hr />

<p>Photo by Annabella</p>

<p>Aha! Look! What a big shark. I&rsquo;m fighting &hellip;</p>

<p><img src="/images/img_for_2015_12_30/bigshark.png" alt="images" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning With Boosting]]></title>
    <link href="http://jasonleaster.github.io/blog/2015/12/13/machine-learning-with-boosting/"/>
    <updated>2015-12-13T17:00:47+08:00</updated>
    <id>http://jasonleaster.github.io/blog/2015/12/13/machine-learning-with-boosting</id>
    <content type="html"><![CDATA[<p>This blog will talk about the theory and implementation about famouse
concept in machine learning &ndash; <code>Boosting</code>.</p>

<p>All algorithms are implemented in Python.</p>

<p>There are two main tasks which people want to finish with Machine Learning.</p>

<ul>
<li>Classification</li>
<li>Regression</li>
</ul>


<p>There are a lot of other ways to do it but now we focus on <code>boosting</code> algorithm. You know that it&rsquo;s a fantastic way to make our work done.</p>

<h3>Adaboost for classification</h3>

<p>If you never hear about adaboost, I recommend you to finish the 7-th lab in MIT 6.034. It will help you a lot to understand what I&rsquo;m taking about. But this lab didn&rsquo;t build adaboost completely. So, I implement it individually.</p>

<p>Give the input training samples which have tag with it.</p>

<p><img src="/images/img_for_2015_12_13/equation.png" alt="images" /></p>

<p>where x[i] is the feature of the i-th sample point and y[i] is the <code>label</code> (soemtimes we call it as <code>tag</code>) with the sample point.</p>

<p>In this algorithm, there are only two different label of samples {-1, +1}.</p>

<p>Some classifier like decision tree also can work correctly about classification. But it&rsquo;s also easy to overfitting. So, we can&rsquo;t use it in some special situation. Instread of using decision tree, we use <code>decision stump</code> which is a special type of decision tree which&rsquo;s depth is only one. So we call it as <code>decision stump</code>.</p>

<p><code>Yoav Freund</code> and <code>Robert Schapire</code> create this algorithm <strong>AdaBoost</strong> which means adaptive boosting. It combine some weaker classifier into a stronger classifier to avoid overfitting.</p>

<p>Test case:</p>

<p>There are training points with two different label. What if we input a point which&rsquo;s type is unkown, what the result will be?</p>

<p><img src="/images/img_for_2015_12_13/samples.png" alt="images" /></p>

<p>The test result is below there:</p>

<p><img src="/images/img_for_2015_12_13/result.png" alt="images" /></p>

<p>Just create a object of class <code>Adaboost</code> with your training samples with label. like this:</p>

<pre><code class="Python">import adaboost
a = AdaBoost(Original_Data, Tag)
# The up bound of training time to avoid the algorithm won't stop for not meeting the training accuracy.
times = 5 

a.train(times)

a.prediction(UnkownPoints)
</code></pre>

<p>API <code>prediction()</code> of class AdaBoost will return the result of prediction according to the model. All job done.</p>

<p>You could find other test case in my repository in github.</p>

<p><a href="https://github.com/jasonleaster/Machine_Learning/tree/master/Adaboost">Implementation of Adaboost in Python</a></p>

<p>You can check the validity of my implementation by the accuracy.</p>

<p><img src="/images/img_for_2015_12_13/samples2.png" alt="images" /></p>

<p>The data used to train the weaker classifier and get that accuracy is showed below there :</p>

<p><img src="/images/img_for_2015_12_13/accuracy.png" alt="images" /></p>

<h3>Boosting Tree</h3>

<p>We have knew to use <code>AdaBoost</code> to do classification. <code>Boosting Tree</code> will help us to do regression.</p>

<p>We also use decision stump as the weak classifier. But implementation of decision stump in this algorithm is not the same as that in AdaBoost.</p>

<p>There are ten samples in my test module:</p>

<pre><code class="python">Original_Data = numpy.array([
        [2],
        [3],
        [4],
        [5],
        [6],
        [7],
        [8],
        [9],
        [10],
        [1]
        ]).transpose()

ExpVal = numpy.array([
        [5.70],
        [5.91],
        [6.40],
        [6.80],
        [7.05],
        [8.90],
        [8.70],
        [9.00],
        [9.05],
        [5.56]
        ]).transpose()
</code></pre>

<p>The expected value of Original_Data[i] is ExpVal[i]. The input is from 1 to 10. How about to predict the output when the input is 1 or 11?</p>

<p>Let&rsquo;s test it. Here is the result:
<img src="/images/img_for_2015_12_13/output_of_boosting_tree.png" alt="images" /></p>

<p>Just used 11 weak classifier to construct a stronger classifier to do the regressio. The output is reasonable.</p>

<p>Here is my implementation of <code>Boosting Tree</code>
<a href="https://github.com/jasonleaster/Machine_Learning/tree/master/Boosting_Tree">Implementation of Boosting Tree in Python</a></p>

<p>Reference:</p>

<ol>
<li>MIT-6.034, Artificial Intelligence. Lab-7</li>
<li>&lt;&lt; The statistic methods >> by HangLi.</li>
<li><a href="https://en.wikipedia.org/wiki/AdaBoost">Wikipedia</a></li>
</ol>


<hr />

<p>Photo by Jason Leaster</p>

<p><img src="/images/img_for_2015_12_13/street.png" alt="images" /></p>
]]></content>
  </entry>
  
</feed>
